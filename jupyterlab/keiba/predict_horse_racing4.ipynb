{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from urllib.request import urlopen\n",
    "import optuna.integration.lightgbm as lgb_o\n",
    "from scipy.special import comb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### クラス定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データと出馬表データを加工する抽象クラス\n",
    "# 追加有\n",
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        self.data = pd.DataFrame()\n",
    "        self.data_p = pd.DataFrame()  # preprocessing後の変数\n",
    "        self.data_h = pd.DataFrame()  # horse_resultsをmergeした後の変数\n",
    "        self.data_pe = pd.DataFrame()  # pedsをmergeした後の変数\n",
    "        self.data_c = pd.DataFrame()  # カテゴリ変数化した後の変数\n",
    "        \n",
    "    # 馬の過去成績データの追加\n",
    "    # 追加有\n",
    "    def merge_horse_results(self, hr, n_samples_list=[5, 9, 'all']):\n",
    "        self.data_h = self.data_p.copy()\n",
    "        for n_samples in n_samples_list:\n",
    "            self.data_h = hr.merge_all(self.data_h, n_samples=n_samples)\n",
    "        # 4/6馬の出走間隔追加\n",
    "        self.data_h['interval'] = (self.data_h['date'] - self.data_h['latest']).dt.days\n",
    "        self.data_h.drop(['開催', 'latest'], axis=1, inplace=True)\n",
    "            \n",
    "    # 血統データ追加\n",
    "    def merge_peds(self, peds):\n",
    "        self.data_pe = self.data_h.merge(peds, left_on='horse_id', right_index=True, how='left')\n",
    "        self.no_peds = self.data_pe[self.data_pe['peds_0'].isnull()]['horse_id'].unique()\n",
    "        if len(self.no_peds):\n",
    "            print('scrape peds at horse_id_list \"no_peds\"')\n",
    "    \n",
    "    # カテゴリ変数の処理\n",
    "    def process_categorical(self, le_horse, le_jockey, results_m):\n",
    "        df = self.data_pe.copy()\n",
    "        \n",
    "        # ラベルエンコーディング(horse_id, jockey_idを0始まりの整数に変換)\n",
    "        # horse_id\n",
    "        mask_horse = df['horse_id'].isin(le_horse.classes_)\n",
    "        new_horse_id = df['horse_id'].mask(mask_horse).dropna().unique()\n",
    "        le_horse.classes_ = np.concatenate([le_horse.classes_, new_horse_id])\n",
    "        df['horse_id'] = le_horse.transform(df['horse_id'])\n",
    "        \n",
    "        # jockey_id\n",
    "        mask_jockey = df['jockey_id'].isin(le_jockey.classes_)\n",
    "        new_jockey_id = df['jockey_id'].mask(mask_jockey).dropna().unique()\n",
    "        le_jockey.classes_ = np.concatenate([le_jockey.classes_, new_jockey_id])\n",
    "        df['jockey_id'] = le_jockey.transform(df['jockey_id'])\n",
    "        \n",
    "        # horse_id, jockey_idをcategory型に変換\n",
    "        df['horse_id'] = df['horse_id'].astype('category')\n",
    "        df['jockey_id'] = df['jockey_id'].astype('category')\n",
    "        \n",
    "        # そのほかのカテゴリ変数をcategory型に変換してからダミー変数化\n",
    "        weathers = results_m['weather'].unique()\n",
    "        race_types = results_m['race_type'].unique()\n",
    "        ground_states = results_m['ground_state'].unique()\n",
    "        sexes = results_m['性'].unique()\n",
    "        df['weather'] = pd.Categorical(df['weather'], weathers)\n",
    "        df['race_type'] = pd.Categorical(df['race_type'], race_types)\n",
    "        df['ground_state'] = pd.Categorical(df['ground_state'], ground_states)\n",
    "        df['性'] = pd.Categorical(df['性'], sexes)\n",
    "        \n",
    "        df = pd.get_dummies(df, columns=['weather', 'race_type', 'ground_state', '性'])  \n",
    "        self.data_c = df\n",
    "\n",
    "# 予測に使う出馬表データを加工するクラス\n",
    "# 追加有\n",
    "class ShutubaTable(DataProcessor):\n",
    "    def __init__(self, shutuba_tables):\n",
    "        super(ShutubaTable, self).__init__()\n",
    "        self.data = shutuba_tables\n",
    "    \n",
    "    # 出馬表データをスクレイピング\n",
    "    # 追加有\n",
    "    @classmethod\n",
    "    def scrape(cls, race_id_list, date):\n",
    "        data = pd.DataFrame()\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            url = f'https://race.netkeiba.com/race/shutuba.html?race_id={race_id}'\n",
    "            df = pd.read_html(url)[0]\n",
    "            df = df.T.reset_index(level=0, drop=True).T\n",
    "\n",
    "            html = requests.get(url)\n",
    "            html.encoding = 'EUC-JP'\n",
    "            soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "            texts = soup.find('div', attrs={'class': 'RaceData01'}).text\n",
    "            texts = re.findall(r'\\w+', texts)\n",
    "            for text in texts:\n",
    "                # 4/6 -1に修正\n",
    "                if 'm' in text:\n",
    "                    df['course_len'] = [int(re.findall(r'\\d+', text)[-1])] * len(df)\n",
    "                if text in ['曇', '晴', '雨', '小雨', '小雪', '雪']:\n",
    "                    df['weather'] = [text] * len(df)\n",
    "                if text in ['良', '稍重', '重']:\n",
    "                    df['ground_state'] = [text] * len(df)\n",
    "                if '不' in text:\n",
    "                    df['ground_state'] = ['不良'] * len(df)\n",
    "                # 4/6追加\n",
    "                if '稍' in text:\n",
    "                    df['ground_state'] = ['稍重'] * len(df)\n",
    "                if '芝' in text:\n",
    "                    df['race_type'] = ['芝'] * len(df)\n",
    "                if '障' in text:\n",
    "                    df['race_type'] = ['障害'] * len(df)\n",
    "                if 'ダ' in text:\n",
    "                    df['race_type'] = ['ダート'] * len(df)\n",
    "            df['date'] = [date] * len(df)\n",
    "\n",
    "            # horse_id\n",
    "            horse_id_list = []\n",
    "            horse_td_list = soup.find_all('td', attrs={'class': 'HorseInfo'})\n",
    "            for td in horse_td_list:\n",
    "                horse_id = re.findall(r'\\d+', td.find('a')['href'])[0]\n",
    "                horse_id_list.append(horse_id)\n",
    "            df['horse_id'] = horse_id_list\n",
    "\n",
    "            # jockey_id\n",
    "            jockey_id_list = []\n",
    "            jockey_td_list = soup.find_all('td', attrs={'class': 'Jockey'})\n",
    "            for td in jockey_td_list:\n",
    "                jockey_id = re.findall(r'\\d+', td.find('a')['href'])[0]\n",
    "                jockey_id_list.append(jockey_id)\n",
    "            df['jockey_id'] = jockey_id_list\n",
    "\n",
    "            df.index = [race_id] * len(df)\n",
    "            data = data.append(df)\n",
    "            time.sleep(1)\n",
    "        \n",
    "        return cls(data)\n",
    "    \n",
    "    # 前処理\n",
    "    # 追加有\n",
    "    def preprocessing(self):\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        # 性齢を性と年齢に分ける\n",
    "        df['性'] = df['性齢'].map(lambda x: str(x)[0])\n",
    "        df['年齢'] = df['性齢'].map(lambda x: str(x)[1:]).astype(int)\n",
    "\n",
    "        # 馬体重を体重と体重変化に分ける\n",
    "        df = df[df['馬体重(増減)'] != '--']\n",
    "        df['体重'] = df['馬体重(増減)'].str.split('(', expand=True)[0].astype(int)\n",
    "        df['体重変化'] = df['馬体重(増減)'].str.split('(', expand=True)[1].str[:-1].astype(int)\n",
    "        # 4/6追加：増減が「前計不」などのときは欠損値にする\n",
    "        df['体重変化'] = pd.to_numeric(df['体重変化'], errors='coerce')\n",
    "        \n",
    "        # 日付をdatetime型に変更\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "        # データをint, floatに変換\n",
    "        df['枠'] = df['枠'].astype(int)\n",
    "        df['馬番'] = df['馬番'].astype(int)\n",
    "        df['斤量'] = df['斤量'].astype(int)\n",
    "        \n",
    "        # 4/6追加：開催場所\n",
    "        df['開催'] = df.index.map(lambda x: str(x)[4:6])\n",
    "        \n",
    "        # 4/6追加：距離は10の位を切り捨てる\n",
    "        df['course_len'] = df['course_len'].astype(float) // 100\n",
    "        \n",
    "        # 4/6追加：出走数\n",
    "        df['n_horses'] = df.index.map(df.index.value_counts())\n",
    "    \n",
    "        # 必要な列だけにする\n",
    "        df = df[['枠', '馬番', '斤量', 'course_len', 'weather', 'race_type', 'ground_state', 'date', 'horse_id', 'jockey_id', '性', '年齢', '体重', '体重変化', '開催', 'n_horses']]\n",
    "        df['開催'] = df.index.map(lambda x:str(x)[4:6])\n",
    "\n",
    "        self.data_p = df.rename(columns={'枠': '枠番'})\n",
    "\n",
    "# 訓練に使う過去レースデータを加工するクラス\n",
    "# 追加有\n",
    "class Results(DataProcessor):\n",
    "    def __init__(self, results):\n",
    "        super(Results, self).__init__()\n",
    "        self.data = results\n",
    "    \n",
    "    # pickleファイルの読み込みと結合\n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.read_pickle(path_list[0])\n",
    "        for path in path_list[1:]:\n",
    "            df = update_data(df, pd.read_pickle(path))\n",
    "        return cls(df)\n",
    "    \n",
    "    # レース結果データをスクレイピング\n",
    "    # 追加有\n",
    "    @staticmethod\n",
    "    def scrape(race_id_list):\n",
    "        # race_idをkeyにしてDataFrame型を格納\n",
    "        race_results = {}\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            try:\n",
    "                url = f'https://db.netkeiba.com/race/{race_id}'\n",
    "                # メインとなるテーブルデータを取得\n",
    "                df = pd.read_html(url)[0]\n",
    "\n",
    "                html = requests.get(url)\n",
    "                html.encoding = 'EUC-JP'\n",
    "                soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "                # 天候、レースの種類、コースの長さ、馬場の状態、日付をスクレイピング\n",
    "                texts = (\n",
    "                    soup.find('div', attrs={'class': 'data_intro'}).find_all('p')[0].text\n",
    "                    + soup.find('div', attrs={'class': 'data_intro'}).find_all('p')[1].text\n",
    "                )\n",
    "                info = re.findall(r'\\w+', texts)\n",
    "                for text in info:\n",
    "                    if text in ['芝', 'ダート']:\n",
    "                        df['race_type'] = [text] * len(df)\n",
    "                    if '障' in text:\n",
    "                        df['race_type'] = ['障害'] * len(df)\n",
    "                    # 4/6 -1に修正\n",
    "                    if 'm' in text:\n",
    "                        df['course_len'] = [int(re.findall(r'\\d+', text)[-1])] * len(df)\n",
    "                    if text in ['良', '稍重', '重', '不良']:\n",
    "                        df['ground_state'] = [text] * len(df)\n",
    "                    if text in ['曇', '晴', '雨', '小雨', '小雪', '雪']:\n",
    "                        df['weather'] = [text] * len(df)\n",
    "                    if '年' in text:\n",
    "                        df['date'] = [text] * len(df)\n",
    "\n",
    "                # 馬ID、騎手IDをスクレイピング\n",
    "                horse_id_list = []\n",
    "                horse_a_list = soup.find('table', attrs={'summary': 'レース結果'}).find_all('a', attrs={'href': re.compile('^/horse')})\n",
    "                for a in horse_a_list:\n",
    "                    horse_id = re.findall(r'\\d+', a['href'])\n",
    "                    horse_id_list.append(horse_id[0])\n",
    "                jockey_id_list = []\n",
    "                jockey_a_list = soup.find(\"table\", attrs={'summary': 'レース結果'}).find_all('a', attrs={'href': re.compile('^/jockey')})\n",
    "                for a in jockey_a_list:\n",
    "                    jockey_id = re.findall(r'\\d+', a['href'])\n",
    "                    jockey_id_list.append(jockey_id[0])\n",
    "                df['horse_id'] = horse_id_list\n",
    "                df['jockey_id'] = jockey_id_list\n",
    "\n",
    "                #インデックスをrace_idにする\n",
    "                df.index = [race_id] * len(df)\n",
    "\n",
    "                race_results[race_id] = df\n",
    "                time.sleep(1)\n",
    "            # 存在しないrace_idを飛ばす\n",
    "            except IndexError:\n",
    "                continue\n",
    "            # 存在しないrace_idでAttributeErrorになるページの対処\n",
    "            except AttributeError:\n",
    "                continue\n",
    "            # wifiの接続が切れた時などでも途中までのデータを返せるようにする\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            # Jupyterで停止ボタンを押した時の対処\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        # pd.DataFrame型にして一つのデータにまとめる\n",
    "        race_results_df = pd.concat([race_results[key] for key in race_results])\n",
    "\n",
    "        return race_results_df\n",
    "    \n",
    "    # 前処理\n",
    "    # 追加有\n",
    "    def preprocessing(self):\n",
    "        df = self.data.copy()\n",
    "\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "        df['rank'] = df['着順'].map(lambda x: 1 if x < 4 else 0)\n",
    "\n",
    "        # 性齢を性と年齢に分ける\n",
    "        df['性'] = df['性齢'].map(lambda x: str(x)[0])\n",
    "        df['年齢'] = df['性齢'].map(lambda x: str(x)[1:]).astype(int)\n",
    "\n",
    "        # 馬体重を体重と体重変化に分ける\n",
    "        df['体重'] = df['馬体重'].str.split('(', expand=True)[0].astype(int)\n",
    "        df['体重変化'] = df['馬体重'].str.split('(', expand=True)[1].str[:-1].astype(int)\n",
    "\n",
    "        # データをint, floatに変換\n",
    "        df['単勝'] = df['単勝'].astype(float)\n",
    "        # 距離は10の位を切り捨てる\n",
    "        df['course_len'] = df['course_len'].astype(float) // 100\n",
    "\n",
    "        # 不要な列を削除\n",
    "        df.drop(['タイム', '着差', '調教師', '性齢', '馬体重', '馬名', '騎手', '人気', '着順'], axis=1, inplace=True)\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%Y年%m月%d日')\n",
    "        \n",
    "        # 開催場所\n",
    "        df['開催'] = df.index.map(lambda x: str(x)[4:6])\n",
    "        \n",
    "        # 6/6出走数追加\n",
    "        df['n_horses'] = df.index.map(df.index.value_counts())\n",
    "\n",
    "        self.data_p = df\n",
    "    \n",
    "    # カテゴリ変数の処理\n",
    "    def process_categorical(self):\n",
    "        self.le_horse = LabelEncoder().fit(self.data_pe['horse_id'])\n",
    "        self.le_jockey = LabelEncoder().fit(self.data_pe['jockey_id'])        \n",
    "        super().process_categorical(self.le_horse, self.le_jockey, self.data_pe)\n",
    "\n",
    "# 馬の過去成績データを処理するクラス\n",
    "# 追加有\n",
    "class HorseResults:\n",
    "    def __init__(self, horse_results):\n",
    "        self.horse_results = horse_results[['日付', '着順', '賞金', '着差', '通過', '開催', '距離']]\n",
    "        self.preprocessing()\n",
    "    \n",
    "    # pickleファイルの読み込みと結合\n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.read_pickle(path_list[0])\n",
    "        for path in path_list[1:]:\n",
    "            df = update_data(df, pd.read_pickle(path))\n",
    "        return cls(df)\n",
    "    \n",
    "    # 馬の過去成績データをスクレイピング\n",
    "    @staticmethod\n",
    "    def scrape(horse_id_list):\n",
    "        # horse_idをkeyにしてDataFrame型を格納\n",
    "        horse_results = {}\n",
    "        for horse_id in tqdm(horse_id_list):\n",
    "            try:\n",
    "                url = f'https://db.netkeiba.com/horse/{horse_id}'\n",
    "                # メインとなるテーブルデータを取得\n",
    "                df = pd.read_html(url)[3]\n",
    "                # 受賞歴がある馬の場合、4番目に受賞歴テーブルが来るため、5番目のデータを取得する\n",
    "                if df.columns[0] == '受賞歴':\n",
    "                    df = pd.read_html(url)[4]\n",
    "                df.index = [horse_id] * len(df)\n",
    "                horse_results[horse_id] = df\n",
    "                time.sleep(1)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        # pd.DataFrame型にして一つのデータにまとめる\n",
    "        horse_results_df = pd.concat([horse_results[key] for key in horse_results])\n",
    "        \n",
    "        return horse_results_df\n",
    "    \n",
    "    # 前処理\n",
    "    def preprocessing(self):\n",
    "        df = self.horse_results.copy()\n",
    "        \n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "        \n",
    "        df['date'] = pd.to_datetime(df['日付'])\n",
    "        df.drop(['日付'], axis=1, inplace=True)\n",
    "        \n",
    "        # 賞金の欠損値を0で埋める\n",
    "        df['賞金'].fillna(0, inplace=True)\n",
    "        \n",
    "        # 1着の着差を0にする\n",
    "        df['着差'] = df['着差'].map(lambda x: 0 if x < 0 else x)\n",
    "                                            \n",
    "        # レース展開データ\n",
    "        def corner(x, n):\n",
    "            if type(x) != str:\n",
    "                return x\n",
    "            # n=1: 最初のコーナー位置、n=4: 最終コーナー位置\n",
    "            if n == 1:\n",
    "                return int(re.findall(r'\\d+', x)[0])\n",
    "            if n == 4:\n",
    "                return int(re.findall(r'\\d+', x)[-1])\n",
    "                                            \n",
    "        df['first_corner'] = df['通過'].map(lambda x: corner(x, 1))\n",
    "        df['final_corner'] = df['通過'].map(lambda x: corner(x, 4))\n",
    "        \n",
    "        df['final_to_rank'] = df['final_corner'] - df['着順']\n",
    "        df['first_to_rank'] = df['first_corner'] - df['着順']\n",
    "        df['first_to_final'] = df['first_corner'] - df['final_corner']\n",
    "        \n",
    "        # 開催場所\n",
    "        df['開催'] = df['開催'].str.extract(r'(\\D+)')[0].map(place_dict).fillna('11')\n",
    "        # race_type\n",
    "        df['race_type'] = df['距離'].str.extract(r'(\\D+)')[0].map(race_type_dict)\n",
    "        # 距離は10の位を切り捨てる\n",
    "        df['course_len'] = df['距離'].str.extract(r'(\\d+)').astype(int) // 100\n",
    "        df.drop(['距離'], axis=1, inplace=True)\n",
    "        \n",
    "        # インデックス名を与える\n",
    "        df.index.name = 'horse_id'\n",
    "        \n",
    "        self.horse_results = df\n",
    "        self.target_list = ['着順', '賞金', '着差', 'first_corner', 'final_corner', 'final_to_rank', 'first_to_rank', 'first_to_final']\n",
    "    \n",
    "    # n_samplesレース分馬ごとに平均する\n",
    "    # 追加有\n",
    "    def average(self, horse_id_list, date, n_samples='all'):\n",
    "        target_df = self.horse_results.query('index in @horse_id_list')\n",
    "        \n",
    "        # 過去何走分取り出すか指定\n",
    "        if n_samples == 'all':\n",
    "            filtered_df = target_df[target_df['date'] < date]\n",
    "        elif n_samples > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n",
    "        else:\n",
    "            raise Exception('n_samples must be integer')\n",
    "        \n",
    "        # 集計して辞書型に入れる\n",
    "        self.average_dict = {}\n",
    "        self.average_dict['non_category'] = filtered_df.groupby(level=0)[self.target_list].mean().add_suffix(f'_{n_samples}R')\n",
    "        for column in ['course_len', 'race_type', '開催']:\n",
    "            self.average_dict[column] = filtered_df.groupby(['horse_id', column])[self.target_list].mean().add_suffix(f'_{column}_{n_samples}R')\n",
    "        \n",
    "        # 4/6追加：全レースの日付を変数latestに格納\n",
    "        if n_samples == 5:\n",
    "            self.latest = filtered_df.groupby('horse_id')['date'].max().rename('latest')\n",
    "    \n",
    "    # 追加有\n",
    "    def merge(self, results, date, n_samples='all'):\n",
    "        df = results[results['date'] == date]\n",
    "        horse_id_list = df['horse_id']\n",
    "        self.average(horse_id_list, date, n_samples)\n",
    "        merged_df = df.merge(self.average_dict['non_category'], left_on='horse_id', right_index=True, how='left')\n",
    "        for column in ['course_len', 'race_type', '開催']:\n",
    "            merged_df = merged_df.merge(self.average_dict[column], left_on=['horse_id', column], right_index=True, how='left')\n",
    "            \n",
    "        # 4/6追加：全レースの日付を変数latestに格納\n",
    "        if n_samples == 5:\n",
    "            merged_df = merged_df.merge(self.latest, left_on='horse_id', right_index=True, how='left')\n",
    "            \n",
    "        return merged_df\n",
    "    \n",
    "    def merge_all(self, results, n_samples='all'):\n",
    "        date_list = results['date'].unique()\n",
    "        merged_df = pd.concat([self.merge(results, date, n_samples) for date in tpdm(date_list)])\n",
    "        return merged_df\n",
    "\n",
    "# 血統データを処理するクラス\n",
    "class Peds:\n",
    "    def __init__(self, peds):\n",
    "        self.peds = peds\n",
    "        self.peds_e = pd.DataFrame()  # LabelEncodingしてcategory型にした変数\n",
    "    \n",
    "    # pickleファイルの読み込みと結合\n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.read_pickle(path_list[0])\n",
    "        for path in path_list[1:]:\n",
    "            df = update_data(df, pd.read_pickle(path))\n",
    "        return cls(df)\n",
    "    \n",
    "    # 血糖データをスクレイピング\n",
    "    @staticmethod\n",
    "    def scrape(horse_id_list):\n",
    "        peds_dict = {}\n",
    "        for horse_id in tqdm(horse_id_list):\n",
    "            try:\n",
    "                url = f'https://db.netkeiba.com/horse/ped/{horse_id}'\n",
    "                df = pd.read_html(url)[0]\n",
    "\n",
    "                # 重複を削除して1列のSeries型データに治す\n",
    "                generations = {}\n",
    "                for i in reversed(range(5)):\n",
    "                    generations[i] = df[i]\n",
    "                    df.drop([i], axis=1, inplace=True)\n",
    "                    df = df.drop_duplicates()\n",
    "\n",
    "                ped = pd.concat([generations[i] for i in range(5)]).rename(horse_id)\n",
    "                peds_dict[horse_id] = ped.reset_index(drop=True)\n",
    "                time.sleep(1)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        # 列名をpeds_0, ..., peds_61にする\n",
    "        peds_df = pd.concat([peds_dict[key] for key in peds_dict], axis=1).T.add_prefix('peds_')\n",
    "        \n",
    "        return peds_df\n",
    "    \n",
    "    def encode(self):\n",
    "        df = self.peds.copy()\n",
    "        for column in df.columns:\n",
    "            df[column] = LabelEncoder().fit_transform(df[column].fillna('Na'))\n",
    "        self.peds_e = df.astype('category')\n",
    "\n",
    "# 払い戻し表データを加工するクラス\n",
    "class Return:\n",
    "    def __init__(self, return_tables):\n",
    "        self.return_tables = return_tables\n",
    "    \n",
    "    # pickleファイルの読み込みと結合\n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.read_pickle(path_list[0])\n",
    "        for path in path_list[1:]:\n",
    "            df = update_data(df, pd.read_pickle(path))\n",
    "        return cls(df)\n",
    "    \n",
    "    # 払い戻し表データをスクレイピング\n",
    "    @staticmethod\n",
    "    def scrape(race_id_list):\n",
    "        return_tables = {}\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            try:\n",
    "                url = f'https://db.netkeiba.com/race/{race_id}'\n",
    "\n",
    "                # 複勝やワイドなどが区切られてしまうため、改行コードを文字列に変換して後でsplitする\n",
    "                f = urlopen(url)\n",
    "                html = f.read()\n",
    "                html = html.replace(b'<br />', b'br')            \n",
    "                dfs = pd.read_html(html)\n",
    "\n",
    "                # dfsの2番目に単勝～馬連、3番目にワイド～3連単がある\n",
    "                df = pd.concat([dfs[1], dfs[2]])\n",
    "                \n",
    "                df.index = [race_id] * len(df)\n",
    "                return_tables[race_id] = df\n",
    "                time.sleep(1)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        # pd.DataFrame型にして一つのデータにまとめる\n",
    "        return_tables_df = pd.concat([return_tables[key] for key in return_tables])\n",
    "        \n",
    "        return return_tables_df\n",
    "    \n",
    "    # 複勝の勝ち馬と払い戻しのデータを取り出す\n",
    "    @property\n",
    "    def fukusho(self):\n",
    "        fukusho = self.return_tables[self.return_tables[0] == '複勝'][[1, 2]]\n",
    "        \n",
    "        # 勝ち馬\n",
    "        wins = fukusho[1].str.split('br', expand=True)[[0, 1, 2]]\n",
    "        wins.columns = ['win_0', 'win_1', 'win_2']\n",
    "        \n",
    "        # 払い戻し\n",
    "        returns = fukusho[2].str.split('br', expand=True)[[0, 1, 2]]\n",
    "        returns.columns = ['return_0', 'return_1', 'return_2']\n",
    "        \n",
    "        df = pd.concat([wins, returns], axis=1)\n",
    "        \n",
    "        # int型に変換できないデータがあるので処理\n",
    "        for column in df.columns:\n",
    "            df[column] = df[column].str.replace(',', '')\n",
    "            \n",
    "        df = df.fillna(0).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    # 単勝の勝ち馬と払い戻しのデータを取り出す\n",
    "    @property\n",
    "    def tansho(self):\n",
    "        tansho = self.return_tables[self.return_tables[0] == '単勝'][[1, 2]]\n",
    "        tansho.columns = ['win', 'return']\n",
    "        \n",
    "        for column in tansho.columns:\n",
    "            tansho[column] = pd.to_numeric(tansho[column], errors='coerce')\n",
    "            \n",
    "        return tansho\n",
    "    \n",
    "    # 馬連の勝ち馬と払い戻しのデータを取り出す\n",
    "    @property\n",
    "    def umaren(self):\n",
    "        umaren = self.return_tables[self.return_tables[0] == '馬連'][[1, 2]]\n",
    "        wins = umaren[1].str.split('-', expand=True)[[0, 1]].add_prefix('win_')\n",
    "        return_ = umaren[2].rename('return')\n",
    "        df = pd.concat([wins, return_], axis=1)      \n",
    "        df = df.apply(lambda x: pd.to_numeric(x, errors='coerce'))\n",
    "        \n",
    "        return df\n",
    "\n",
    "# 予測モデルを評価するクラス\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, model, return_tables_path_list):\n",
    "        self.model = model\n",
    "        self.rt = Return.read_pickle(return_tables_path_list)\n",
    "        self.fukusho = self.rt.fukusho\n",
    "        self.tansho = self.rt.tansho\n",
    "        self.umaren = self.rt.umaren\n",
    "    \n",
    "    # 1（3着以内）になる確率を予測\n",
    "    def predict_proba(self, x, std=True, minmax=False):\n",
    "        proba = pd.Series(self.model.predict_proba(x.drop(['単勝'], axis=1))[:, 1], index=x.index)\n",
    "        if std:\n",
    "            # 標準化：レース内で相対評価する\n",
    "            standard_scaler = lambda x: (x - x.mean()) / x.std()\n",
    "            proba = proba.groupby(level=0).transform(standard_scaler)\n",
    "        if minmax:\n",
    "            # MinMaxスケーリング：データを0から1にする\n",
    "            proba = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "        return proba\n",
    "    \n",
    "    # thresholdの値によってpredict_probaで出した値を0か1に決定する\n",
    "    def predict(self, x, threshold=0.5):\n",
    "        y_pred = self.predict_proba(x)\n",
    "        return [0 if p < threshold else 1 for p in y_pred]\n",
    "    \n",
    "    # スコアを表示\n",
    "    def score(self, y_true, x):\n",
    "        return roc_auc_score(y_true, self.predict_proba(x))\n",
    "    \n",
    "    # 変数の重要度を出力\n",
    "    def feature_importance(self, x, n_display=20):\n",
    "        importances = pd.DataFrame(\n",
    "            {\"features\": x.columns, \"importance\": self.model.feature_importances_}\n",
    "        )\n",
    "        return importances.sort_values('importance', ascending=False)[:n_display]\n",
    "    \n",
    "    # 予測したレースIDと馬番を出力する\n",
    "    def pred_table(self, x, threshold=0.5, bet_only=True):\n",
    "        pred_table = x.copy()[['馬番', '単勝']]\n",
    "        pred_table['pred'] = self.predict(x, threshold)\n",
    "        \n",
    "        if bet_only:\n",
    "            return pred_table[pred_table['pred'] == 1][['馬番', '単勝']]\n",
    "        \n",
    "        return pred_table\n",
    "    \n",
    "    # 複勝の払い戻しを表示\n",
    "    def fukusho_return(self, x, threshold=0.5):\n",
    "        pred_table = self.pred_table(x, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        money = -100 * n_bets\n",
    "        df = self.fukusho.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        for i in range(3):\n",
    "            money += df[df[f'win_{i}'] == df['馬番']][f'return_{i}'].sum()\n",
    "        return_rate = (n_bets * 100 + money) / (n_bets * 100)\n",
    "        return n_bets, return_rate\n",
    "    \n",
    "    # 単勝の払い戻しを表示\n",
    "    def tansho_return(self, x, threshold=0.5):\n",
    "        pred_table = self.pred_table(x, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        n_races = pred_table.index.nunique()\n",
    "        \n",
    "        money = -100 * n_bets\n",
    "        df = self.tansho.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        \n",
    "        std = ((df['win'] == df['馬番']) * df['return']).groupby(level=0).sum().std() * np.sqrt(n_races) / (100 * n_bets)\n",
    "        \n",
    "        n_hits = len(df[df['win'] == df['馬番']])\n",
    "        money += df[df['win'] == df['馬番']]['return'].sum()\n",
    "        return_rate = (n_bets * 100 + money) / (n_bets * 100)\n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    # モデルによって「賭ける」と判断された馬たち\n",
    "    def tansho_return_proper(self, x, threshold=0.5):\n",
    "        pred_table = self.pred_table(x, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        n_races = pred_table.index.nunique()\n",
    "        \n",
    "        # 払い戻し表にpred_tableをマージ\n",
    "        df = self.tansho.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        \n",
    "        bet_money = (1 / pred_table['単勝']).sum()\n",
    "        \n",
    "        std = ((df['win'] == df['馬番']).astype(int)).groupby(level=0).sum().std() * np.sqrt(n_races) / bet_money\n",
    "        \n",
    "        # 単勝適正回収値を計算\n",
    "        n_hits = len(df.query('win == 馬番'))\n",
    "        return_rate = n_hits / bet_money\n",
    "        \n",
    "        return n_bets, return_rate, n_hits, std\n",
    "    \n",
    "    # 馬連の払い戻しを表示\n",
    "    def umaren_return(self, x, threshold=0.5):\n",
    "        pred_table = self.pred_table(x, threshold)\n",
    "        hit = {}\n",
    "        n_bets = 0\n",
    "        for race_id, preds in pred_table.groupby(level=0):\n",
    "            n_bets += comb(len(preds, 2))\n",
    "            hit[race_id] = set(self.umaren.loc[race_id][['win_0', 'win_1']]).issubset(set(preds))\n",
    "        return_rate = self.umaren.index.map(hit).values * self.umaren['return'].sum() / (n_bets * 100)\n",
    "        return n_bets, return_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### その他関数等定義"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時系列に沿って訓練データとテストデータに分ける関数\n",
    "def split_data(df, test_size=0.3):\n",
    "    sorted_id_list = df.sort_values('date').index.unique()   \n",
    "    train_id_list = sorted_id_list[:round(len(sorted_id_list) * (1 - test_size))]\n",
    "    test_id_list = sorted_id_list[round(len(sorted_id_list) * (1 - test_size)):]\n",
    "    train = df.loc[train_id_list]\n",
    "    test = df.loc[test_id_list]\n",
    "    return train, test\n",
    "\n",
    "# 回収率を計算する関数\n",
    "def gain(return_func, x, n_samples=100, t_range=[0.5, 3.5]):\n",
    "    gain = {}\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        # min_thresholdから1まで、n_samples等分して、thresholdをfor文で回す\n",
    "        threshold = t_range[1] * i / n_samples + t_range[0] * (1 - (i / n_samples))\n",
    "        n_bets, return_rate, n_hits, std = return_func(x, threshold)\n",
    "        if n_bets > 2:\n",
    "            gain[threshold] = {'return_rate': return_rate, 'n_hits': n_hits, 'std': std, 'n_bets': n_bets}\n",
    "    return pd.DataFrame(gain).T\n",
    "\n",
    "# 重複のないデータを作成する関数\n",
    "def update_data(old, new):\n",
    "    filtered_old = old[~old.index.isin(new.index)]\n",
    "    return pd.concat([filtered_old, new])\n",
    "\n",
    "# 標準偏差付き回収率プロット\n",
    "def plot(df, label=' '):\n",
    "    # 標準偏差で幅をつけて薄くプロット\n",
    "    plt.fill_between(df.index, y1=df['return_rate']-df['std'], y2=df['return_rate']+df['std'], alpha=0.3)\n",
    "    # 回収率で実践をプロット\n",
    "    plt.plot(df.index, df['return_rate'], label=label)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "# 開催場所をidに変換するための辞書型\n",
    "place_dict = {\n",
    "    '札幌': '01', '函館': '02', '福島': '03', '新潟': '04', '東京': '05',\n",
    "    '中山': '06', '中京': '07', '京都': '08', '阪神': '09', '小倉': '10'\n",
    "}\n",
    "\n",
    "# レースタイプをレース結果データと整合させるための辞書型\n",
    "race_type_dict = {\n",
    "    '芝': '芝', 'ダ': 'ダート', '障': '障害'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2017年～2021年までのレース結果データをスクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2017年のレースIDリスト\n",
    "# race_id_list_2017 = []\n",
    "# for place in range(1, 11):\n",
    "#     for kai in range(1, 6):\n",
    "#         for day in range(1, 13):\n",
    "#             for r in range(1, 13):\n",
    "#                 race_id = f'2017{str(place).zfill(2)}{str(kai).zfill(2)}{str(day).zfill(2)}{str(r).zfill(2)}'\n",
    "#                 race_id_list_2017.append(race_id)\n",
    "\n",
    "# 2018年のレースIDリスト\n",
    "# race_id_list_2018 = []\n",
    "# for place in range(1, 11):\n",
    "#     for kai in range(1, 6):\n",
    "#         for day in range(1, 13):\n",
    "#             for r in range(1, 13):\n",
    "#                 race_id = f'2018{str(place).zfill(2)}{str(kai).zfill(2)}{str(day).zfill(2)}{str(r).zfill(2)}'\n",
    "#                 race_id_list_2018.append(race_id)\n",
    "\n",
    "# 2019年のレースIDリスト\n",
    "# race_id_list_2019 = []\n",
    "# for place in range(1, 11):\n",
    "#     for kai in range(1, 6):\n",
    "#         for day in range(1, 13):\n",
    "#             for r in range(1, 13):\n",
    "#                 race_id = f'2019{str(place).zfill(2)}{str(kai).zfill(2)}{str(day).zfill(2)}{str(r).zfill(2)}'\n",
    "#                 race_id_list_2019.append(race_id)\n",
    "\n",
    "# 2020年のレースIDリスト\n",
    "# race_id_list_2020 = []\n",
    "# for place in range(1, 11):\n",
    "#     for kai in range(1, 6):\n",
    "#         for day in range(1, 13):\n",
    "#             for r in range(1, 13):\n",
    "#                 race_id = f'2020{str(place).zfill(2)}{str(kai).zfill(2)}{str(day).zfill(2)}{str(r).zfill(2)}'\n",
    "#                 race_id_list_2020.append(race_id)\n",
    "\n",
    "# 2021年のレースIDリスト\n",
    "# race_id_list_2021 = []\n",
    "# for place in range(1, 11):\n",
    "#     for kai in range(1, 6):\n",
    "#         for day in range(1, 13):\n",
    "#             for r in range(1, 13):\n",
    "#                 race_id = f'2021{str(place).zfill(2)}{str(kai).zfill(2)}{str(day).zfill(2)}{str(r).zfill(2)}'\n",
    "#                 race_id_list_2021.append(race_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクレイピングしたのでコメント\n",
    "\n",
    "# 2017年のレース結果データ\n",
    "# results_2017 = Results.scrape(race_id_list)\n",
    "# results_2017.to_pickle('pickle/results_2017.pickle')\n",
    "# results_2017 = pd.read_pickle('pickle/results_2017.pickle')\n",
    "\n",
    "# 2018年のレース結果データ\n",
    "# results_2018 = Results.scrape(race_id_list_2018)\n",
    "# results_2018.to_pickle('pickle/results_2018.pickle')\n",
    "# results_2018 = pd.read_pickle('pickle/results_2018.pickle')\n",
    "\n",
    "# 2019年のレース結果データ\n",
    "# results_2019 = Results.scrape(race_id_list_2019)\n",
    "# results_2019.to_pickle('pickle/results_2019.pickle')\n",
    "# results_2019 = pd.read_pickle('pickle/results_2019.pickle')\n",
    "\n",
    "# 2020年のレース結果データ\n",
    "# results_2020 = Results.scrape(race_id_list_2020)\n",
    "# results_2020.to_pickle('pickle/results_2020.pickle')\n",
    "# results_2020 = pd.read_pickle('pickle/results_2020.pickle')\n",
    "\n",
    "# 2021年のレース結果データ\n",
    "# results_2021 = Results.scrape(race_id_list_2021)\n",
    "# results_2021.to_pickle('pickle/results_2021.pickle')\n",
    "# results_2021 = pd.read_pickle('pickle/results_2021.pickle')\n",
    "\n",
    "# レース結果データの結合\n",
    "# r = Results.read_pickle(['pickle/results_2017.pickle', 'pickle/results_2018.pickle', 'pickle/results_2019.pickle', 'pickle/results_2020.pickle', 'pickle/results_2021.pickle'])\n",
    "\n",
    "# 前処理\n",
    "# r.preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2017～2021年までの馬の過去成績データをスクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2017年の馬IDリスト\n",
    "# horse_id_list_2017 = results_2017['horse_id'].unique()\n",
    "\n",
    "# 2018年の馬IDリスト\n",
    "# horse_id_list_2018 = results_2018['horse_id'].unique()\n",
    "\n",
    "# 2019年の馬IDリスト\n",
    "# horse_id_list_2019 = results_2019['horse_id'].unique()\n",
    "\n",
    "# 2020年の馬IDリスト\n",
    "# horse_id_list_2020 = results_2020['horse_id'].unique()\n",
    "\n",
    "# 2021年の馬IDリスト\n",
    "# horse_id_list_2021 = results_2021['horse_id'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクレイピングして結合したのでコメント\n",
    "\n",
    "# 2017年の馬の過去成績データ\n",
    "# horse_results_2017 = HorseResults.scrape(horse_id_list_2017)\n",
    "# horse_results_2017.to_pickle('pickle/horse_results_2017.pickle')\n",
    "\n",
    "# 2018年の馬の過去成績データ\n",
    "# horse_results_2018 = HorseResults.scrape(horse_id_list_2018)\n",
    "# horse_results_2018.to_pickle('pickle/horse_results_2018.pikcle')\n",
    "\n",
    "# 2019年の馬の過去成績データ\n",
    "# horse_results_2019 = HorseResults.scrape(horse_id_list_2019)\n",
    "# horse_results_2019.to_pickle('pickle/horse_results_2019.pikcle')\n",
    "\n",
    "# 2020年の馬の過去成績データ\n",
    "# horse_results_2020 = HorseResults.scrape(horse_id_list_2020)\n",
    "# horse_results_2020.to_pickle('pickle/horse_results_2020.pikcle')\n",
    "\n",
    "# 2021年の馬の過去成績データ\n",
    "# horse_results_2021 = HorseResults.scrape(horse_id_list_2021)\n",
    "# horse_results_2021.to_pickle('pickle/horse_results_2021.pickle')\n",
    "\n",
    "# 馬の過去成績データの結合\n",
    "# hr = HorseResults.read_pickle(['pickle/horse_results_2017.pickle', 'pickle/horse_results_2018.pickle', 'pickle/horse_results_2019.pickle', 'pickle/horse_results_2020.pickle', 'pickle/horse_results_2021.pickle'])\n",
    "\n",
    "# レース結果データに馬の過去成績データを追加\n",
    "# r.merge_horse_results(hr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2017年～2021年までの血統データをスクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクレイピングして結合したのでコメント\n",
    "\n",
    "# 2017年の血統データ\n",
    "# peds_2017 = Peds.scrape(horse_id_list_2017)\n",
    "# peds_2017.to_pickle('pickle/peds_2017.pickle')\n",
    "\n",
    "# 2018年の血統データ\n",
    "# peds_2018 = Peds.scrape(horse_id_list_2018)\n",
    "# peds_2018.to_pickle('pickle/peds_2018.pickle')\n",
    "\n",
    "# 2019年の血統データ\n",
    "# peds_2019 = Peds.scrape(horse_id_list_2019)\n",
    "# peds_2019.to_pickle('pickle/peds_2019.pickle')\n",
    "\n",
    "# 2020年の血統データ\n",
    "# peds_2020 = Peds.scrape(horse_id_list_2020)\n",
    "# peds_2020.to_pickle('pickle/peds_2020.pickle')\n",
    "\n",
    "# 2021年の血統データ\n",
    "# peds_2021 = Peds.scrape(horse_id_list_2021)\n",
    "# peds_2021.to_pickle('pickle/peds_2021.pickle')\n",
    "\n",
    "# 血統データの結合\n",
    "# p = Peds.read_pickle(['pickle/peds_2017.pickle', 'pickle/peds_2018.pickle', 'pickle/peds_2019.pickle', 'pickle/peds_2020.pickle', 'pickle/peds_2021.pickle'])\n",
    "\n",
    "# レース結果データに5世代分の血統データを追加\n",
    "# p.encode()\n",
    "# r.merge_peds(p.peds_e)\n",
    "\n",
    "# カテゴリ変数の処理\n",
    "# r.process_categorical()\n",
    "\n",
    "# pickleファイルに保存\n",
    "# r.data_c.to_pickle('pickle/results_all_c.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_c = pd.read_pickle('pickle/results_all_c.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2017年～2021年の払い戻し表データをスクレイピング"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# レース結果データから取得した2017年のレースIDリスト\n",
    "# race_id_list_2017 = results_2017.index.unique()\n",
    "\n",
    "# レース結果データから取得した2018年のレースIDリスト\n",
    "# race_id_list_2018 = results_2018.index.unique()\n",
    "\n",
    "# レース結果データから取得した2019年のレースIDリスト\n",
    "# race_id_list_2019 = results_2019.index.unique()\n",
    "\n",
    "# レース結果データから取得した2020年のレースIDリスト\n",
    "# race_id_list_2020 = results_2020.index.unique()\n",
    "\n",
    "# レース結果データから取得した2021年のレースIDリスト\n",
    "# race_id_list_2021 = results_2021.index.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# スクレイピングして結合したのでコメント\n",
    "\n",
    "# 2017年の払い戻し表データ\n",
    "# return_tables_2017 = Return.scrape(race_id_list_2017)\n",
    "# return_tables_2017.to_pickle('pickle/return_tables_2017.pickle')\n",
    "\n",
    "# 2018年の払い戻し表データ\n",
    "# return_tables_2018 = Return.scrape(race_id_list_2018)\n",
    "# return_tables_2018.to_pickle('pickle/return_tables_2018.pickle')\n",
    "\n",
    "# 2019年の払い戻し表データ\n",
    "# return_tables_2019 = Return.scrape(race_id_list_2019)\n",
    "# return_tables_2019.to_pickle('pickle/return_tables_2019.pickle')\n",
    "\n",
    "# 2020年の払い戻し表データ\n",
    "# return_tables_2020 = Return.scrape(race_id_list_2020)\n",
    "# return_tables_2020.to_pickle('pickle/return_tables_2020.pickle')\n",
    "\n",
    "# 2021年の払い戻し表データ\n",
    "# return_tables_2021 = Return.scrape(race_id_list_2021)\n",
    "# return_tables_2021.to_pickle('pickle/return_tables_2021.pickle')\n",
    "\n",
    "# 払い戻し表データの結合\n",
    "# return_tables = Return.read_pickle(['pickle/return_tables_2017.pickle', 'pickle/return_tables_2018.pickle', 'pickle/return_tables_2019.pickle', 'pickle/return_tables_2020.pickle', 'pickle/return_tables_2021.pickle'])\n",
    "\n",
    "# pickleファイルに保存\n",
    "# return_tables.return_tables.to_pickle('pickle/return_tables.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "return_tables = pd.read_pickle('pickle/return_tables.pickle')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学習"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時系列に沿って訓練データとテストデータに分ける\n",
    "train, test = split_data(results_c)\n",
    "X_train = train.drop(['rank', 'date', '単勝'], axis=1)\n",
    "y_train = train['rank']\n",
    "X_test = test.drop(['rank', 'date'], axis=1)\n",
    "y_test = test['rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(bagging_fraction=1.0, bagging_freq=0, feature_fraction=1.0,\n",
       "               feature_pre_filter=False, lambda_l1=1.3132588653273114e-05,\n",
       "               lambda_l2=7.718638255265974, num_leaves=33, objective='binary',\n",
       "               random_state=100)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# optunaでパラメータチューニングしたパラメータ\n",
    "params = {\n",
    " 'objective': 'binary',\n",
    " 'random_state': 100,\n",
    " 'feature_pre_filter': False,\n",
    " 'lambda_l1': 1.3132588653273114e-05,\n",
    " 'lambda_l2': 7.718638255265974,\n",
    " 'num_leaves': 33,\n",
    " 'feature_fraction': 1.0,\n",
    " 'bagging_fraction': 1.0,\n",
    " 'bagging_freq': 0,\n",
    " 'min_child_samples': 20\n",
    "}\n",
    "\n",
    "# LGBMで学習\n",
    "lgb_clf = lgb.LGBMClassifier(**params)\n",
    "lgb_clf.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ModelEvaluatorクラスのオブジェクトを作成\n",
    "me = ModelEvaluator(lgb_clf, ['pickle/return_tables.pickle'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 98%|█████████▊| 98/100 [07:14<00:09,  4.64s/it]"
     ]
    }
   ],
   "source": [
    "g_tansho = gain(me.tansho_return, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot(g_tansho, 'tansho')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
