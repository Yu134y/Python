{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1．インポート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-fc8513b891c6>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-fc8513b891c6>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    from bs4 import Beautifu lSoup\u001b[0m\n\u001b[1;37m                                 ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import time\n",
    "import datetime\n",
    "import requests\n",
    "from tqdm import tqdm\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "from urllib.request import urlopen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1　クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練データと出馬表データを加工する抽象クラス\n",
    "class DataProcessor:\n",
    "    def __init__(self):\n",
    "        self.data = pd.DataFrame()\n",
    "        # preprocessing後の変数\n",
    "        self.data_p = pd.DataFrame()\n",
    "        # horse_resultsをmergeした後の変数\n",
    "        self.data_h = pd.DataFrame()\n",
    "        # pedsをmergeした後の変数\n",
    "        self.data_pe = pd.DataFrame()\n",
    "        # カテゴリ変数化した後の変数\n",
    "        self.data_c = pd.DataFrame()\n",
    "        \n",
    "    # 馬の過去成績データの追加\n",
    "    def merge_horse_results(self, hr, n_samples_list=[5, 9, 'all']):\n",
    "        self.data_h = self.data_p.copy()\n",
    "        for n_samples in n_samples_list:\n",
    "            self.data_h = hr.merge_all(self.data_h, n_samples=n_samples)\n",
    "            \n",
    "    # 血統データ追加\n",
    "    def merge_peds(self, peds):\n",
    "        self.data_pe = self.data_h.merge(peds, left_on='horse_id', right_index=True, how='left')\n",
    "        self.no_peds = self.data_pe[self.data_pe['peds_0'].isnull()]['horse_id'].unique()\n",
    "        if len(self.no_peds):\n",
    "            print('scrape peds at horse_id_list \"no_peds\"')\n",
    "    \n",
    "    # カテゴリ変数の処理\n",
    "    def process_categorical(self, le_horse, le_jockey, results_m):\n",
    "        df = self.data_pe.copy()\n",
    "        \n",
    "        # ラベルエンコーディング(horse_id, jockey_idを0始まりの整数に変換)\n",
    "        # horse_id\n",
    "        mask_horse = df['horse_id'].isin(le_horse.classes_)\n",
    "        new_horse_id = df['horse_id'].mask(mask_horse).dropna().unique()\n",
    "        le_horse.classes_ = np.concatenate([le_horse.classes_, new_horse_id])\n",
    "        df['horse_id'] = le_horse.transform(df['horse_id'])\n",
    "        \n",
    "        # jockey_id\n",
    "        mask_jockey = df['jockey_id'].isin(le_jockey.classes_)\n",
    "        new_jockey_id = df['jockey_id'].mask(mask_jockey).dropna().unique()\n",
    "        le_jockey.classes_ = np.concatenate([le_jockey.classes_, new_jockey_id])\n",
    "        df['jockey_id'] = le_jockey.transform(df['jockey_id'])\n",
    "        \n",
    "        # horse_id, jockey_idをcategory型に変換\n",
    "        df['horse_id'] = df['horse_id'].astype('category')\n",
    "        df['jockey_id'] = df['jockey_id'].astype('category')\n",
    "        \n",
    "        # そのほかのカテゴリ変数をcategory型に変換してからダミー変数化\n",
    "        weathers = results_m['weather'].unique()\n",
    "        race_types = results_m['race_type'].unique()\n",
    "        ground_states = results_m['ground_state'].unique()\n",
    "        sexes = results_m['性'].unique()\n",
    "        df['weather'] = pd.Categorical(df['weather'], weathers)\n",
    "        df['race_type'] = pd.Categorical(df['race_type'], race_types)\n",
    "        df['ground_state'] = pd.Categorical(df['ground_state'], ground_states)\n",
    "        df['性'] = pd.Categorical(df['性'], sexes)\n",
    "        \n",
    "        df = pd.get_dummies(df, columns=['weather', 'race_type', 'ground_state', '性'])  \n",
    "        self.data_c = df\n",
    "\n",
    "# 予測に使う出馬表データを加工するクラス\n",
    "class ShutubaTable(DataProcessor):\n",
    "    def __init__(self, shutuba_tables):\n",
    "        super(ShutubaTable, self).__init__()\n",
    "        self.data = shutuba_tables\n",
    "    \n",
    "    @classmethod\n",
    "    def scrape(cls, race_id_list, date):\n",
    "        data = pd.DataFrame()\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            url = f'https://race.netkeiba.com/race/shutuba.html?race_id={race_id}'\n",
    "            df = pd.read_html(url)[0]\n",
    "            df = df.T.reset_index(level=0, drop=True).T\n",
    "\n",
    "            html = requests.get(url)\n",
    "            html.encoding = 'EUC-JP'\n",
    "            soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "            # race_data\n",
    "            texts = soup.find('div', attrs={'class': 'RaceData01'}).text\n",
    "            texts = re.findall(r'\\w+', texts)\n",
    "            for text in texts:\n",
    "                if 'm' in text:\n",
    "                    df['course_len'] = [int(re.findall(r'\\d+', text)[0])] * len(df)\n",
    "                if text in ['曇', '晴', '雨', '小雨', '小雪', '雪']:\n",
    "                    df['weather'] = [text] * len(df)\n",
    "                if text in ['良', '稍重', '重']:\n",
    "                    df['ground_state'] = [text] * len(df)\n",
    "                if '不' in text:\n",
    "                    df['ground_state'] = ['不良'] * len(df)\n",
    "                if '芝' in text:\n",
    "                    df['race_type'] = ['芝'] * len(df)\n",
    "                if '障' in text:\n",
    "                    df['race_type'] = ['障害'] * len(df)\n",
    "                if 'ダ' in text:\n",
    "                    df['race_type'] = ['ダート'] * len(df)\n",
    "            df['date'] = [date] * len(df)\n",
    "\n",
    "            # horse_id\n",
    "            horse_id_list = []\n",
    "            horse_td_list = soup.find_all('td', attrs={'class': 'HorseInfo'})\n",
    "            for td in horse_td_list:\n",
    "                horse_id = re.findall(r'\\d+', td.find('a')['href'])[0]\n",
    "                horse_id_list.append(horse_id)\n",
    "            df['horse_id'] = horse_id_list\n",
    "\n",
    "            # jockey_id\n",
    "            jockey_id_list = []\n",
    "            jockey_td_list = soup.find_all('td', attrs={'class': 'Jockey'})\n",
    "            for td in jockey_td_list:\n",
    "                jockey_id = re.findall(r'\\d+', td.find('a')['href'])[0]\n",
    "                jockey_id_list.append(jockey_id)\n",
    "            df['jockey_id'] = jockey_id_list\n",
    "\n",
    "            df.index = [race_id] * len(df)\n",
    "            data = data.append(df)\n",
    "            time.sleep(1)\n",
    "        \n",
    "        return cls(data)\n",
    "    \n",
    "    # 前処理\n",
    "    def preprocessing(self):\n",
    "        df = self.data.copy()\n",
    "        \n",
    "        # 性齢を性と年齢に分ける\n",
    "        df['性'] = df['性齢'].map(lambda x: str(x)[0])\n",
    "        df['年齢'] = df['性齢'].map(lambda x: str(x)[1:]).astype(int)\n",
    "\n",
    "        # 馬体重を体重と体重変化に分ける\n",
    "        df = df[df['馬体重(増減)'] != '--']\n",
    "        df['体重'] = df['馬体重(増減)'].str.split('(', expand=True)[0].astype(int)\n",
    "        df['体重変化'] = df['馬体重(増減)'].str.split('(', expand=True)[1].str[:-1].astype(int)\n",
    "        \n",
    "        # 日付をdatetime型に変更\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "        # データをint, floatに変換\n",
    "        df['枠'] = df['枠'].astype(int)\n",
    "        df['馬番'] = df['馬番'].astype(int)\n",
    "        df['斤量'] = df['斤量'].astype(int)\n",
    "\n",
    "    \n",
    "        # 必要な列だけにする\n",
    "        df = df[['枠', '馬番', '斤量', 'course_len', 'weather', 'race_type', 'ground_state', 'date', 'horse_id', 'jockey_id', '性', '年齢', '体重', '体重変化']]\n",
    "\n",
    "        self.data_p = df.rename(columns={'枠': '枠番'})\n",
    "\n",
    "# 訓練に使う過去レースデータを加工するクラス\n",
    "class Results(DataProcessor):\n",
    "    def __init__(self, results):\n",
    "        super(Results, self).__init__()\n",
    "        self.data = results\n",
    "    \n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.read_pickle(path_list[0])\n",
    "        for path in path_list[1:]:\n",
    "            df = update_data(df, pd.read_pickle(path))\n",
    "        return cls(df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def scrape(race_id_list):\n",
    "        # race_idをkeyにしてDataFrame型を格納\n",
    "        race_results = {}\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            time.sleep(1)\n",
    "            try:\n",
    "                url = f'https://db.netkeiba.com/race/{race_id}'\n",
    "                # メインとなるテーブルデータを取得\n",
    "                df = pd.read_html(url)[0]\n",
    "\n",
    "                html = requests.get(url)\n",
    "                html.encoding = 'EUC-JP'\n",
    "                soup = BeautifulSoup(html.text, 'html.parser')\n",
    "\n",
    "                # 天候、レースの種類、コースの長さ、馬場の状態、日付をスクレイピング\n",
    "                texts = (\n",
    "                    soup.find('div', attrs={'class': 'data_intro'}).find_all('p')[0].text\n",
    "                    + soup.find('div', attrs={'class': 'data_intro'}).find_all('p')[1].text\n",
    "                )\n",
    "                info = re.findall(r'\\w+', texts)\n",
    "                for text in info:\n",
    "                    if text in ['芝', 'ダート']:\n",
    "                        df['race_type'] = [text] * len(df)\n",
    "                    if '障' in text:\n",
    "                        df['race_type'] = ['障害'] * len(df)\n",
    "                    if 'm' in text:\n",
    "                        df['course_len'] = [int(re.findall(r'\\d+', text)[0])] * len(df)\n",
    "                    if text in ['良', '稍重', '重', '不良']:\n",
    "                        df['ground_state'] = [text] * len(df)\n",
    "                    if text in ['曇', '晴', '雨', '小雨', '小雪', '雪']:\n",
    "                        df['weather'] = [text] * len(df)\n",
    "                    if '年' in text:\n",
    "                        df['date'] = [text] * len(df)\n",
    "\n",
    "                # 馬ID、騎手IDをスクレイピング\n",
    "                horse_id_list = []\n",
    "                horse_a_list = soup.find('table', attrs={'summary': 'レース結果'}).find_all('a', attrs={'href': re.compile('^/horse')})\n",
    "                for a in horse_a_list:\n",
    "                    horse_id = re.findall(r'\\d+', a['href'])\n",
    "                    horse_id_list.append(horse_id[0])\n",
    "                jockey_id_list = []\n",
    "                jockey_a_list = soup.find(\"table\", attrs={'summary': 'レース結果'}).find_all('a', attrs={'href': re.compile('^/jockey')})\n",
    "                for a in jockey_a_list:\n",
    "                    jockey_id = re.findall(r'\\d+', a['href'])\n",
    "                    jockey_id_list.append(jockey_id[0])\n",
    "                df['horse_id'] = horse_id_list\n",
    "                df['jockey_id'] = jockey_id_list\n",
    "\n",
    "                #インデックスをrace_idにする\n",
    "                df.index = [race_id] * len(df)\n",
    "\n",
    "                race_results[race_id] = df\n",
    "            # 存在しないrace_idを飛ばす\n",
    "            except IndexError:\n",
    "                continue\n",
    "            # wifiの接続が切れた時などでも途中までのデータを返せるようにする\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            # Jupyterで停止ボタンを押した時の対処\n",
    "            except:\n",
    "                break\n",
    "\n",
    "        # pd.DataFrame型にして一つのデータにまとめる\n",
    "        race_results_df = pd.concat([race_results[key] for key in race_results])\n",
    "\n",
    "        return race_results_df\n",
    "    \n",
    "    # 前処理\n",
    "    def preprocessing(self):\n",
    "        df = self.data.copy()\n",
    "\n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df = df[~(df['着順'].astype(str).str.contains('\\D'))]\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "        df['rank'] = df['着順'].map(lambda x: 1 if x < 4 else 0)\n",
    "\n",
    "        # 性齢を性と年齢に分ける\n",
    "        df['性'] = df['性齢'].map(lambda x: str(x)[0])\n",
    "        df['年齢'] = df['性齢'].map(lambda x: str(x)[1:]).astype(int)\n",
    "\n",
    "        # 馬体重を体重と体重変化に分ける\n",
    "        df['体重'] = df['馬体重'].str.split('(', expand=True)[0].astype(int)\n",
    "        df['体重変化'] = df['馬体重'].str.split('(', expand=True)[1].str[:-1].astype(int)\n",
    "\n",
    "        # データをint, floatに変換\n",
    "        df['単勝'] = df['単勝'].astype(float)\n",
    "        df['course_len'] = df['course_len'].astype(int)\n",
    "\n",
    "        # 不要な列を削除\n",
    "        df.drop(['タイム', '着差', '調教師', '性齢', '馬体重', '馬名', '騎手', '単勝', '人気', '着順'], axis=1, inplace=True)\n",
    "\n",
    "        df['date'] = pd.to_datetime(df['date'], format='%Y年%m月%d日')\n",
    "\n",
    "        self.data_p = df\n",
    "    \n",
    "    # カテゴリ変数の処理\n",
    "    def process_categorical(self):\n",
    "        self.le_horse = LabelEncoder().fit(self.data_pe['horse_id'])\n",
    "        self.le_jockey = LabelEncoder().fit(self.data_pe['jockey_id'])        \n",
    "        super().process_categorical(self.le_horse, self.le_jockey, self.data_pe)\n",
    "\n",
    "# 馬の過去成績データを処理するクラス\n",
    "class HorseResults:\n",
    "    def __init__(self, horse_results):\n",
    "        self.horse_results = horse_results[['日付', '着順', '賞金']]\n",
    "        self.preprocessing()\n",
    "    \n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.read_pickle(path_list[0])\n",
    "        for path in path_list[1:]:\n",
    "            df = update_data(df, pd.read_pickle(path))\n",
    "        return cls(df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def scrape(horse_id_list):\n",
    "        # horse_idをkeyにしてDataFrame型を格納\n",
    "        horse_results = {}\n",
    "        for horse_id in tqdm(horse_id_list):\n",
    "            try:\n",
    "                time.sleep(1)\n",
    "                url = f'https://db.netkeiba.com/horse/{horse_id}'\n",
    "                # メインとなるテーブルデータを取得\n",
    "                df = pd.read_html(url)[3]\n",
    "                # 受賞歴がある馬の場合、4番目に受賞歴テーブルが来るため、5番目のデータを取得する\n",
    "                if df.columns[0] == '受賞歴':\n",
    "                    df = pd.read_html(url)[4]\n",
    "                horse_results[horse_id] = df\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        # pd.DataFrame型にして一つのデータにまとめる\n",
    "        horse_results_df = pd.concat([horse_results[key] for key in horse_results])\n",
    "        \n",
    "        return horse_results_df\n",
    "        \n",
    "    def preprocessing(self):\n",
    "        df = self.horse_results.copy()\n",
    "        \n",
    "        # 着順に数字以外の文字列が含まれているものを取り除く\n",
    "        df['着順'] = pd.to_numeric(df['着順'], errors='coerce')\n",
    "        df.dropna(subset=['着順'], inplace=True)\n",
    "        df['着順'] = df['着順'].astype(int)\n",
    "        \n",
    "        df['date'] = pd.to_datetime(df['日付'])\n",
    "        df.drop(['日付'], axis=1, inplace=True)\n",
    "        \n",
    "        # 賞金の欠損値を0で埋める\n",
    "        df['賞金'].fillna(0, inplace=True)\n",
    "        \n",
    "        self.horse_results = df\n",
    "    \n",
    "    def average(self, horse_id_list, date, n_samples='all'):\n",
    "        target_df = self.horse_results.query('index in @horse_id_list')\n",
    "        \n",
    "        # 過去何走分取り出すか指定\n",
    "        if n_samples == 'all':\n",
    "            filtered_df = target_df[target_df['date'] < date]\n",
    "        elif n_samples > 0:\n",
    "            filtered_df = target_df[target_df['date'] < date].sort_values('date', ascending=False).groupby(level=0).head(n_samples)\n",
    "        else:\n",
    "            raise Exception('n_samples must be integer')\n",
    "            \n",
    "        average = filtered_df.groupby(level=0)[['着順', '賞金']].mean()\n",
    "        rename_columns = average.rename(columns={'着順': f'着順_{n_samples}R', '賞金': f'賞金_{n_samples}R'})\n",
    "        return rename_columns\n",
    "    \n",
    "    def merge(self, results, date, n_samples='all'):\n",
    "        df = results[results['date'] == date]\n",
    "        horse_id_list = df['horse_id']\n",
    "        merged_df = df.merge(self.average(horse_id_list, date, n_samples), left_on='horse_id', right_index=True, how='left')\n",
    "        return merged_df\n",
    "    \n",
    "    def merge_all(self, results, n_samples='all'):\n",
    "        date_list = results['date'].unique()\n",
    "        merged_df = pd.concat([self.merge(results, date, n_samples) for date in date_list])\n",
    "        return merged_df\n",
    "\n",
    "# 血統データを処理するクラス\n",
    "class Peds:\n",
    "    def __init__(self, peds):\n",
    "        self.peds = peds\n",
    "        # LabelEncodingしてcategory型にした変数\n",
    "        self.peds_e = pd.DataFrame()\n",
    "    \n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.read_pickle(path_list[0])\n",
    "        for path in path_list[1:]:\n",
    "            df = update_data(df, pd.read_pickle(path))\n",
    "        return cls(df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def scrape(horse_id_list):\n",
    "        # 血統データを取り出す\n",
    "        peds_dict = {}\n",
    "        for horse_id in tqdm(horse_id_list):\n",
    "            try:\n",
    "                url = f'https://db.netkeiba.com/horse/ped/{horse_id}'\n",
    "                df = pd.read_html(url)[0]\n",
    "\n",
    "                # 重複を削除して1列のSeries型データに治す\n",
    "                generations = {}\n",
    "                for i in reversed(range(5)):\n",
    "                    generations[i] = df[i]\n",
    "                    df.drop([i], axis=1, inplace=True)\n",
    "                    df = df.drop_duplicates()\n",
    "\n",
    "                ped = pd.concat([generations[i] for i in range(5)]).rename(horse_id)\n",
    "                peds_dict[horse_id] = ped.reset_index(drop=True)\n",
    "                time.sleep(1)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        # 列名をpeds_0, ..., peds_61にする\n",
    "        peds_df = pd.concat([peds_dict[key] for key in peds_dict], axis=1).T.add_prefix('peds_')\n",
    "        \n",
    "        return peds_df\n",
    "    \n",
    "    def encode(self):\n",
    "        df = self.peds.copy()\n",
    "        for column in df.columns:\n",
    "            df[column] = LabelEncoder().fit_transform(df[column].fillna('Na'))\n",
    "        self.peds_e = df.astype('category')\n",
    "\n",
    "# 払い戻しデータを加工するクラス\n",
    "class Return:\n",
    "    def __init__(self, return_tables):\n",
    "        self.return_tables = return_tables\n",
    "    \n",
    "    @classmethod\n",
    "    def read_pickle(cls, path_list):\n",
    "        df = pd.read_pickle(path_list[0])\n",
    "        for path in path_list[1:]:\n",
    "            df = update_data(df, pd.read_pickle(path))\n",
    "        return cls(df)\n",
    "    \n",
    "    @staticmethod\n",
    "    def scrape(race_id_list):\n",
    "        return_tables = {}\n",
    "        for race_id in tqdm(race_id_list):\n",
    "            try:\n",
    "                url = f'https://db.netkeiba.com/race/{race_id}'\n",
    "\n",
    "                # 複勝やワイドなどが区切られてしまうため、改行コードを文字列に変換して後でsplitする\n",
    "                f = urlopen(url)\n",
    "                html = f.read()\n",
    "                html = html.replace(b'<br />', b'br')            \n",
    "                dfs = pd.read_html(html)\n",
    "\n",
    "                # dfsの2番目に単勝～馬連、3番目にワイド～3連単がある\n",
    "                df = pd.concat([dfs[1], dfs[2]])\n",
    "                \n",
    "                df.index = [race_id] * len(df)\n",
    "                return_tables[race_id] = df\n",
    "                time.sleep(1)\n",
    "            except IndexError:\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                break\n",
    "            except:\n",
    "                break\n",
    "        \n",
    "        # pd.DataFrame型にして一つのデータにまとめる\n",
    "        return_tables_df = pd.concat([return_tables[key] for key in return_tables])\n",
    "        \n",
    "        return return_tables_df\n",
    "    \n",
    "    @property\n",
    "    def fukusho(self):\n",
    "        # 複勝の勝ち馬と払い戻しのデータを取り出す\n",
    "        fukusho = self.return_tables[self.return_tables[0] == '複勝'][[1, 2]]\n",
    "        \n",
    "        # 勝ち馬\n",
    "        wins = fukusho[1].str.split('br', expand=True)[[0, 1, 2]]\n",
    "        wins.columns = ['win_0', 'win_1', 'win_2']\n",
    "        \n",
    "        # 払い戻し\n",
    "        returns = fukusho[2].str.split('br', expand=True)[[0, 1, 2]]\n",
    "        returns.columns = ['return_0', 'return_1', 'return_2']\n",
    "        \n",
    "        df = pd.concat([wins, returns], axis=1)\n",
    "        \n",
    "        # int型に変換できないデータがあるので処理\n",
    "        for column in df.columns:\n",
    "            df[column] = df[column].str.replace(',', '')\n",
    "            \n",
    "        df = df.fillna(0).astype(int)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    @property\n",
    "    def tansho(self):\n",
    "        tansho = self.return_tables[self.return_tables[0] == '単勝'][[1, 2]]\n",
    "        tansho.columns = ['win', 'return']\n",
    "        \n",
    "        for column in tansho.columns:\n",
    "            tansho[column] = pd.to_numeric(tansho[column], errors='coerce')\n",
    "            \n",
    "        return tansho\n",
    "\n",
    "# 予測モデルを評価するクラス\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, model, return_tables_path, std=True):\n",
    "        self.model = model\n",
    "        self.fukusho = Return.read_pickle([return_tables_path]).fukusho\n",
    "        self.tansho = Return.read_pickle([return_tables_path]).tansho\n",
    "        self.std = std\n",
    "        \n",
    "    def predict_proba(self, x):\n",
    "        # 1（3着以内）になる確率を予測\n",
    "        proba = pd.Series(self.model.predict_proba(x)[:, 1], index=x.index)\n",
    "        if self.std:\n",
    "            # 標準化\n",
    "            standard_scaler = lambda x: (x - x.mean()) / x.std()\n",
    "            proba = proba.groupby(level=0).transform(standard_scaler)\n",
    "            # MinMaxスケーリング：データを0から1にする\n",
    "            proba = (proba - proba.min()) / (proba.max() - proba.min())\n",
    "        return proba\n",
    "    \n",
    "    def predict(self, x, threshold=0.5):\n",
    "        # thresholdの値によってpredict_probaで出した値を0か1に決定する\n",
    "        y_pred = self.predict_proba(x)\n",
    "        return [0 if p < threshold else 1 for p in y_pred]\n",
    "    \n",
    "    def score(self, y_true, x):\n",
    "        # スコアを表示\n",
    "        return roc_auc_score(y_true, self.predict_proba(x))\n",
    "    \n",
    "    def feature_importance(self, x, n_display=20):\n",
    "        # 変数の重要度を出力\n",
    "        importances = pd.DataFrame(\n",
    "            {\"features\": x.columns, \"importance\": self.model.feature_importances_}\n",
    "        )\n",
    "        return importances.sort_values('importance', ascending=False)[:n_display]\n",
    "    \n",
    "    def pred_table(self, x, threshold=0.5, bet_only=True):\n",
    "        # 予測したレースIDと馬番を出力する\n",
    "        pred_table = x.copy()[['馬番']]\n",
    "        pred_table['pred'] = self.predict(x, threshold)\n",
    "        \n",
    "        if bet_only:\n",
    "            return pred_table[pred_table['pred'] == 1]['馬番']\n",
    "        \n",
    "        return pred_table\n",
    "    \n",
    "    def fukusho_return(self, x, threshold=0.5):\n",
    "        # 複勝の払い戻しを表示\n",
    "        pred_table = self.pred_table(x, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        money = -100 * n_bets\n",
    "        df = self.fukusho.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        for i in range(3):\n",
    "            money += df[df[f'win_{i}'] == df['馬番']][f'return_{i}'].sum()\n",
    "        return_rate = (n_bets * 100 + money) / (n_bets * 100)\n",
    "        return n_bets, return_rate\n",
    "    \n",
    "    def tansho_return(self, x, threshold=0.5):\n",
    "        # 単勝の払い戻しを表示\n",
    "        pred_table = self.pred_table(x, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        money = -100 * n_bets\n",
    "        df = self.tansho.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        money += df[df['win'] == df['馬番']]['return'].sum()\n",
    "        return_rate = (n_bets * 100 + money) / (n_bets * 100)\n",
    "        return n_bets, return_rate\n",
    "    \n",
    "    def tansho_return_proper(self, X, threshold=0.5):\n",
    "        pred_table = self.pred_table(X, threshold)\n",
    "        n_bets = len(pred_table)\n",
    "        df = self.tansho.copy()\n",
    "        df = df.merge(pred_table, left_index=True, right_index=True, how='right')\n",
    "        \n",
    "        # 単勝適性回収値を計算\n",
    "        return_rate = len(df.query('win == 馬番')) / (100 / df['return']).sum()\n",
    "        \n",
    "        return n_bets, return_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2　関数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 時系列に沿って訓練データとテストデータに分ける関数\n",
    "def split_data(df, test_size=0.3):\n",
    "    sorted_id_list = df.sort_values('date').index.unique()   \n",
    "    train_id_list = sorted_id_list[:round(len(sorted_id_list) * (1 - test_size))]\n",
    "    test_id_list = sorted_id_list[round(len(sorted_id_list) * (1 - test_size)):]\n",
    "    train = df.loc[train_id_list]#.drop(['date'], axis=1)\n",
    "    test = df.loc[test_id_list]#.drop(['date'], axis=1)\n",
    "    return train, test\n",
    "\n",
    "# 回収率を計算する関数\n",
    "def gain(return_func, x, n_samples=100, lower=50, min_threshold=0.5):\n",
    "    gain = {}\n",
    "    for i in tqdm(range(n_samples)):\n",
    "        threshold = (i / n_samples) + (min_threshold * (1 - (i / n_samples)))\n",
    "        n_bets, return_rate = return_func(x, threshold)\n",
    "        if n_bets > lower:\n",
    "            gain[n_bets] = return_rate\n",
    "    return pd.Series(gain)\n",
    "\n",
    "# 重複のないデータを作成する関数\n",
    "def update_data(old, new):\n",
    "    filtered_old = old[~old.index.isin(new.index)]\n",
    "    return pd.concat([filtered_old, new])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2．前処理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = Results.read_pickle(['results.pickle'])\n",
    "\n",
    "# 前処理\n",
    "r.preprocessing()\n",
    "\n",
    "# 馬の過去成績データの追加\n",
    "hr = HorseResults.read_pickle(['horse_results.pickle'])\n",
    "r.merge_horse_results(hr)\n",
    "\n",
    "# 5世代分の血統データの追加\n",
    "p = Peds.read_pickle(['peds.pickle'])\n",
    "p.encode()\n",
    "r.merge_peds(p.peds_e)\n",
    "\n",
    "# カテゴリ変数の処理\n",
    "r.process_categorical()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3．予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_data(r.data_c)\n",
    "X_train = train.drop(['rank', 'date'], axis=1)\n",
    "y_train = train['rank']\n",
    "X_test = test.drop(['rank', 'date'], axis=1)\n",
    "y_test = test['rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'num_leaves': 4,\n",
    "    'n_estimators': 80,\n",
    "    'class_weight': 'balanced',\n",
    "    'random_state': 100\n",
    "}\n",
    "\n",
    "lgb_clf = lgb.LGBMClassifier(**params)\n",
    "lgb_clf.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me = ModelEvaluator(lgb_clf, 'return_tables.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = gain(me.tansho_return, X_test)\n",
    "baseline.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4．単勝適性回収値\n",
    "常に払戻額が一定になるように単勝で賭けた場合の回収率<br>\n",
    "払戻額が常に1万円になるように賭けてみる"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "賭けた金額<br>\n",
    "x円賭けるとすると<br>\n",
    "$$\n",
    "各馬番の単勝オッズ \\times \\frac{x}{100} = 10000 \\\\\n",
    "\\therefore \\quad x = \\frac{100 \\times 10000}{各馬番の単勝オッズ}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回収率 = 払戻額 / 賭けた金額"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_proper = gain(me.tansho_return_proper, X_test)\n",
    "gain_proper.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "baseline.rename('baseline').plot(legend=True)\n",
    "gain_proper.rename('proper').plot(legend=True)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5．Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = split_data(r.data_c)\n",
    "train, valid = split_data(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna.integration.lightgbm as lgb_o\n",
    "\n",
    "# 説明変数と目的変数に分ける\n",
    "X_train = train.drop(['rank', 'date'], axis=1)\n",
    "y_train = train['rank']\n",
    "X_valid = valid.drop(['rank', 'date'], axis=1)\n",
    "y_valid = valid['rank']\n",
    "X_test = test.drop(['rank', 'date'], axis=1)\n",
    "y_test = test['rank']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データセットの作成\n",
    "lgb_train = lgb_o.Dataset(X_train.values, y_train.values)\n",
    "lgb_valid = lgb_o.Dataset(X_valid.values, y_valid.values)\n",
    "\n",
    "# チューニング実行\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'random_state': 100\n",
    "}\n",
    "\n",
    "lgb_clf_o = lgb_o.train(params, lgb_train, valid_sets=(lgb_train, lgb_valid), verbose_eval=100, early_stopping_rounds=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1．あるハイパーパラメータを使って、lgb_trainで学習<br>\n",
    "2．そのモデルを使ってlgb_validで検証<br>\n",
    "3．検証結果を利用して、ハイパーパラメータを変えてlgb_trainで再学習<br>\n",
    "4．2～3を繰り返す"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf_o.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgb_clf = lgb.LGBMClassifier(**lgb_clf_o.params)\n",
    "lgb_clf.fit(X_train.values, y_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "me2 = ModelEvaluator(lgb_clf, return_tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_optuna = gain(me2.tansho_return_proper, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 8))\n",
    "gain_proper.rename('proper').plot(legend=True)\n",
    "gain_optuna.rename('optuna').plot(legend=True)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gain_optuna.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
